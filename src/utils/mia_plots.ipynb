{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first generate the mean, highest, and lowest client lists for each subgroup\n",
    "# supergroup = \"100\" # topology\n",
    "subgroup = \"\" # alpha later\n",
    "base_dir = \"../expt_dump/mia/SGD/\"\n",
    "suffix_dir = \"_cifar10_\"\n",
    "mia_metric = \"correct_prob\" # entropy, loss\n",
    "\n",
    "def collect_experiment_data(base_dir, mia_metric):\n",
    "    experiment_data = {}\n",
    "\n",
    "    for alpha in os.listdir(base_dir):\n",
    "        exp_path = os.path.join(base_dir, alpha)\n",
    "        if os.path.isdir(exp_path):\n",
    "\n",
    "            for exp in os.listdir(exp_path):\n",
    "                topology = exp.split(\"_\")[0]\n",
    "                experiment_data[topology] = {}\n",
    "                # print(f\"Processing {topology} {alpha} {exp}\")\n",
    "                topology_path = os.path.join(exp_path, exp)\n",
    "                log_path = os.path.join(topology_path, \"logs\")\n",
    "\n",
    "                for client in os.listdir(log_path):\n",
    "                    client_path = os.path.join(log_path, client, f\"{mia_metric}_mia_stats_summary.json\")\n",
    "                    print(f\"Processing {client_path}\")\n",
    "\n",
    "                    if os.path.exists(client_path):\n",
    "                        with open(client_path, \"r\") as f:\n",
    "                            metrics.append(json.load(f))\n",
    "\n",
    "                # # Assuming all metrics are lists of equal length\n",
    "                # metrics = np.array(metrics)\n",
    "                # avg_metric = np.mean(metrics, axis=0)\n",
    "                # min_metric = np.min(metrics, axis=0)\n",
    "                # max_metric = np.max(metrics, axis=0)\n",
    "\n",
    "                # experiment_data[exp] = {\n",
    "                #     \"avg\": avg_metric,\n",
    "                #     \"min\": min_metric,\n",
    "                #     \"max\": max_metric,\n",
    "                # }\n",
    "\n",
    "    return experiment_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def collect_experiment_data(base_dir, mia_metric):\n",
    "    \"\"\"\n",
    "    Collect and organize experiment data by both topology and alpha.\n",
    "    Returns two dictionaries:\n",
    "    1. Data organized by alpha -> topology\n",
    "    2. Data organized by topology -> alpha\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries to store data\n",
    "    data_by_alpha = {}  # For task 1: graphs per alpha with different topologies\n",
    "    data_by_topology = {}  # For task 2: graphs per topology with different alphas\n",
    "    \n",
    "    # Process all directories in the base directory (alpha values)\n",
    "    for alpha in sorted(os.listdir(base_dir)):\n",
    "        alpha_path = os.path.join(base_dir, alpha)\n",
    "        if not os.path.isdir(alpha_path):\n",
    "            continue\n",
    "            \n",
    "        # Initialize alpha entry in the data_by_alpha dictionary\n",
    "        data_by_alpha[alpha] = {}\n",
    "        \n",
    "        # Process all experiment directories in the alpha directory\n",
    "        for exp in sorted(os.listdir(alpha_path)):\n",
    "            # Extract topology from experiment name\n",
    "            topology = exp.split(\"_\")[0]\n",
    "            \n",
    "            # Initialize topology in data_by_topology if it doesn't exist\n",
    "            if topology not in data_by_topology:\n",
    "                data_by_topology[topology] = {}\n",
    "                \n",
    "            # Initialize topology in data_by_alpha[alpha] if it doesn't exist\n",
    "            data_by_alpha[alpha][topology] = []\n",
    "            \n",
    "            # Initialize alpha in data_by_topology[topology] if it doesn't exist\n",
    "            data_by_topology[topology][alpha] = []\n",
    "            \n",
    "            # Get path to logs directory\n",
    "            topology_path = os.path.join(alpha_path, exp)\n",
    "            log_path = os.path.join(topology_path, \"logs\")\n",
    "            \n",
    "            # Skip if logs directory doesn't exist\n",
    "            if not os.path.exists(log_path):\n",
    "                print(f\"Logs directory not found: {log_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Process each client directory\n",
    "            client_metrics = []\n",
    "            for client in sorted(os.listdir(log_path)):\n",
    "                client_dir = os.path.join(log_path, client)\n",
    "                \n",
    "                # Skip if not a directory\n",
    "                if not os.path.isdir(client_dir):\n",
    "                    continue\n",
    "                    \n",
    "                # Path to metrics file\n",
    "                metrics_file = os.path.join(client_dir, f\"{mia_metric}_mia_stats_summary.json\")\n",
    "                \n",
    "                # Skip if metrics file doesn't exist\n",
    "                if not os.path.exists(metrics_file):\n",
    "                    print(f\"Metrics file not found: {metrics_file}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Load metrics\n",
    "                try:\n",
    "                    with open(metrics_file, \"r\") as f:\n",
    "                        client_data = json.load(f)\n",
    "                        client_metrics.append(client_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading metrics from {metrics_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Skip if no client metrics were loaded\n",
    "            if not client_metrics:\n",
    "                print(f\"No client metrics found for {topology} in {alpha}\")\n",
    "                continue\n",
    "                \n",
    "            # Process client metrics and store in both dictionaries\n",
    "            data_by_alpha[alpha][topology] = client_metrics\n",
    "            data_by_topology[topology][alpha] = client_metrics\n",
    "    \n",
    "    return data_by_alpha, data_by_topology\n",
    "\n",
    "def process_metrics(client_metrics):\n",
    "    \"\"\"\n",
    "    Process client metrics to get median, min, and max values for each epoch.\n",
    "    Returns epoch numbers, median values, min values, and max values.\n",
    "    \"\"\"\n",
    "    # Convert string epoch keys to integers and sort them\n",
    "    all_epochs = set()\n",
    "    for client_data in client_metrics:\n",
    "        all_epochs.update(int(epoch) for epoch in client_data.keys())\n",
    "    \n",
    "    epochs = sorted(all_epochs)\n",
    "    \n",
    "    # Initialize arrays to store metrics for each epoch\n",
    "    median_values = []\n",
    "    min_values = []\n",
    "    max_values = []\n",
    "    \n",
    "    # Process each epoch\n",
    "    for epoch in epochs:\n",
    "        epoch_str = str(epoch)\n",
    "        # Collect values for this epoch across all clients\n",
    "        epoch_values = []\n",
    "        \n",
    "        for client_data in client_metrics:\n",
    "            if epoch_str in client_data:\n",
    "                epoch_values.append(client_data[epoch_str])\n",
    "        \n",
    "        if epoch_values:\n",
    "            # Calculate median, min, and max\n",
    "            median_values.append(np.median(epoch_values))\n",
    "            min_values.append(np.min(epoch_values))\n",
    "            max_values.append(np.max(epoch_values))\n",
    "        else:\n",
    "            # No data for this epoch, use NaN\n",
    "            median_values.append(np.nan)\n",
    "            min_values.append(np.nan)\n",
    "            max_values.append(np.nan)\n",
    "    \n",
    "    return epochs, median_values, min_values, max_values\n",
    "\n",
    "def create_alpha_graphs(data_by_alpha, mia_metric, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Create one graph per alpha with all topologies overlaid (Task 1).\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up the figure for all alphas\n",
    "    fig, axes = plt.subplots(1, len(data_by_alpha), figsize=(6*len(data_by_alpha), 5), sharey=True)\n",
    "    \n",
    "    # If there's only one alpha, make axes iterable\n",
    "    if len(data_by_alpha) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Color palette for topologies\n",
    "    colors = sns.color_palette(\"husl\", len(set(topology for alpha_data in data_by_alpha.values() for topology in alpha_data)))\n",
    "    topology_colors = {}\n",
    "    \n",
    "    # Get unique topologies across all alphas\n",
    "    all_topologies = sorted(set(topology for alpha_data in data_by_alpha.values() for topology in alpha_data))\n",
    "    for i, topology in enumerate(all_topologies):\n",
    "        topology_colors[topology] = colors[i]\n",
    "    \n",
    "    # Create one graph per alpha\n",
    "    for i, (alpha, topology_data) in enumerate(sorted(data_by_alpha.items())):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Set title and labels\n",
    "        ax.set_title(f\"Alpha = {alpha}\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f\"{mia_metric.replace('_', ' ').title()} ROC AUC\")\n",
    "        \n",
    "        # Plot each topology\n",
    "        for topology, client_metrics in sorted(topology_data.items()):\n",
    "            if not client_metrics:\n",
    "                continue\n",
    "                \n",
    "            epochs, median_values, min_values, max_values = process_metrics(client_metrics)\n",
    "            \n",
    "            # Plot median line\n",
    "            line = ax.plot(epochs, median_values, label=topology, color=topology_colors[topology])\n",
    "            \n",
    "            # Plot min-max region\n",
    "            ax.fill_between(epochs, min_values, max_values, alpha=0.2, color=line[0].get_color())\n",
    "        \n",
    "        # Add legend\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{mia_metric}_by_alpha.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def create_topology_graphs(data_by_topology, mia_metric, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Create one graph per topology with all alphas overlaid (Task 2).\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up the figure for all topologies\n",
    "    fig, axes = plt.subplots(1, len(data_by_topology), figsize=(6*len(data_by_topology), 5), sharey=True)\n",
    "    \n",
    "    # If there's only one topology, make axes iterable\n",
    "    if len(data_by_topology) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Color palette for alphas\n",
    "    colors = sns.color_palette(\"viridis\", len(set(alpha for topology_data in data_by_topology.values() for alpha in topology_data)))\n",
    "    alpha_colors = {}\n",
    "    \n",
    "    # Get unique alphas across all topologies\n",
    "    all_alphas = sorted(set(alpha for topology_data in data_by_topology.values() for alpha in topology_data))\n",
    "    for i, alpha in enumerate(all_alphas):\n",
    "        alpha_colors[alpha] = colors[i]\n",
    "    \n",
    "    # Create one graph per topology\n",
    "    for i, (topology, alpha_data) in enumerate(sorted(data_by_topology.items())):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Set title and labels\n",
    "        ax.set_title(f\"Topology = {topology}\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f\"{mia_metric.replace('_', ' ').title()} ROC AUC\")\n",
    "        \n",
    "        # Plot each alpha\n",
    "        for alpha, client_metrics in sorted(alpha_data.items()):\n",
    "            if not client_metrics:\n",
    "                continue\n",
    "                \n",
    "            epochs, median_values, min_values, max_values = process_metrics(client_metrics)\n",
    "            \n",
    "            # Plot median line\n",
    "            line = ax.plot(epochs, median_values, label=f\"Alpha={alpha}\", color=alpha_colors[alpha])\n",
    "            \n",
    "            # Plot min-max region\n",
    "            ax.fill_between(epochs, min_values, max_values, alpha=0.2, color=line[0].get_color())\n",
    "        \n",
    "        # Add legend\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{mia_metric}_by_topology.png\"), dpi=300)\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
